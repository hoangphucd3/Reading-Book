# Methods

From the earliest days of the industry, advocates of one or another
methodology have promised that it alone holds the key to bringing software
projects in on time, on budget, and with fewer bugs. The problem
with this one-size-fits-all rhetoric is that no one approach could possibly
work across the wide range of software projects. What makes sense for a
product destined for use by millions of consumers (the old shrink-wrapped
software market) probably doesn't make sense for software written to be
used internally at one company. A program destined for use by an entire
government agency makes different demands on its developers than a specialty
product for, say, audio postproduction

 The Joel
Test asks the following dozen questions:
- Do you use source control?
- Can you make a build in one step?
- Do you make daily builds?
- Do you have a bug database?
- Do you fix bugs before writing new code?
- Do you have an up-to-date schedule?
- Do you have a spec?
- Do programmers have quiet working conditions?
- Do you use the best tools money can buy?
- Do you have testers?
- Do new candidates write code during their interview?
- Do you do hallway usability testing? 

"A score of 12 is perfect," Spolsky wrote, "11 is tolerable, but 10 or lower
and you've got serious problems. The truth is that most software organizations
are running with a score of 2 or 3, and they need serious help, because
companies like Microsoft run at 12 full-time"

Despite the odds—despite complexity and delay and unpredictable
change—a lot of software somehow does get written and delivered and,
finally, used. Occasionally, it's even good. Rarely, it actually does something
new and valuable. And in a handful of cases, it achieves all of that on
schedule

Where you find software success stories,
you invariably find people who are good at saying no
